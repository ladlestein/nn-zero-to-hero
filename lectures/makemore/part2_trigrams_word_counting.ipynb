{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the work-counting version of makemore for trigrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "with open(\"names.txt\") as file:\n",
    "    words = file.read().splitlines()\n",
    "words[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode all the words with integer values for the letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = \".abcdefghijklmnopqrstuvwxyz\"\n",
    "char_to_index = {}\n",
    "index = 0\n",
    "for char in list(chars):\n",
    "    char_to_index[char] = index\n",
    "    index += 1\n",
    "\n",
    "index_to_char = {index:char for char,index in char_to_index.items()}\n",
    "all_words_encoded = []\n",
    "for word in words:\n",
    "    letters = [\".\", \".\"] + list(word) + [\".\"]\n",
    "    # letters = [\".\"] + list(word) + [\".\"]\n",
    "    all_words_encoded.append([char_to_index[letter] for letter in letters])\n",
    "\n",
    "def btoi(chars):\n",
    "    if len(chars) != 2:\n",
    "        raise Exception(\"bigram_to_index accepts only strings of two characters where both are either letters or \\\".\\\"\")\n",
    "    return char_to_index[chars[0]] * 27 + char_to_index[chars[1]]\n",
    "#\"\".join([index_to_char[char] for char in encoded_words[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split all the words into trigrams. Represent the input (the first two characters) as the second + 27 * the first. Count the # of occurrences of each and index by the bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([729, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([4.0100e+00, 1.3010e+01, 1.0000e-02, 1.0000e-02, 1.0000e-02, 2.0100e+00,\n",
       "        1.0000e-02, 1.0000e-02, 1.0000e-02, 8.0100e+00, 1.0000e-02, 1.0000e-02,\n",
       "        5.0100e+00, 4.0100e+00, 1.0000e-02, 1.0100e+00, 1.0000e-02, 1.0000e-02,\n",
       "        1.0000e-02, 1.0000e-02, 1.0000e-02, 1.0100e+00, 1.0000e-02, 1.0000e-02,\n",
       "        1.0000e-02, 7.0100e+00, 1.0000e-02], dtype=torch.float64)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "counts = torch.zeros(27*27, 27, dtype=float)\n",
    "counts += .01\n",
    "print(counts.shape)\n",
    "\n",
    "n_observations = 0\n",
    "for word_encoded in all_words_encoded[:]:\n",
    "    for index1, index2, index3 in zip(word_encoded, word_encoded[1:], word_encoded[2:]):\n",
    "        x = (index1 * 27) + index2\n",
    "        y = index3\n",
    "        counts[x,y] += 1\n",
    "        n_observations += 1\n",
    "\n",
    "counts[728]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the counts for each bigram    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = counts / counts.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2268, dtype=torch.float64)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[245, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ola.\n",
      "cliz.\n",
      "rhylileimbron.\n",
      "mionnee.\n",
      "prinie.\n",
      "drition.\n",
      "ryddica.\n",
      "cri.\n",
      "cley.\n",
      "zubliangen.\n",
      "avie.\n",
      "joysi.\n",
      "arliegri.\n",
      "luwavlamehah.\n",
      "ozelyn.\n",
      "ranaelykenzelilen.\n",
      "ryatti.\n",
      "alah.\n",
      "bryahnianal.\n",
      "ell.\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "  output = []\n",
    "  bigram_index = 0 # the bigram representing two word-starts characters.\n",
    "  \n",
    "  while True:\n",
    "    next_index = torch.multinomial(probs[bigram_index], 1, replacement=True).item()\n",
    "    next_char = index_to_char[next_index]\n",
    "    output.append(next_char)\n",
    "    if next_char == \".\":\n",
    "      break\n",
    "    bigram_index = (bigram_index % 27) * 27 + next_index\n",
    "  print(\"\".join(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". tensor(0.0331, dtype=torch.float64)\n",
      "a tensor(0.1653, dtype=torch.float64)\n",
      "b tensor(3.6728e-05, dtype=torch.float64)\n",
      "c tensor(3.6728e-05, dtype=torch.float64)\n",
      "d tensor(0.0037, dtype=torch.float64)\n",
      "e tensor(0.4444, dtype=torch.float64)\n",
      "f tensor(3.6728e-05, dtype=torch.float64)\n",
      "g tensor(0.0037, dtype=torch.float64)\n",
      "h tensor(3.6728e-05, dtype=torch.float64)\n",
      "i tensor(0.1506, dtype=torch.float64)\n",
      "j tensor(3.6728e-05, dtype=torch.float64)\n",
      "k tensor(3.6728e-05, dtype=torch.float64)\n",
      "l tensor(3.6728e-05, dtype=torch.float64)\n",
      "m tensor(3.6728e-05, dtype=torch.float64)\n",
      "n tensor(0.0515, dtype=torch.float64)\n",
      "o tensor(0.0698, dtype=torch.float64)\n",
      "p tensor(3.6728e-05, dtype=torch.float64)\n",
      "q tensor(3.6728e-05, dtype=torch.float64)\n",
      "r tensor(0.0257, dtype=torch.float64)\n",
      "s tensor(3.6728e-05, dtype=torch.float64)\n",
      "t tensor(3.6728e-05, dtype=torch.float64)\n",
      "u tensor(0.0037, dtype=torch.float64)\n",
      "v tensor(3.6728e-05, dtype=torch.float64)\n",
      "w tensor(3.6728e-05, dtype=torch.float64)\n",
      "x tensor(3.6728e-05, dtype=torch.float64)\n",
      "y tensor(0.0478, dtype=torch.float64)\n",
      "z tensor(3.6728e-05, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "bii = btoi(\"yd\")\n",
    "for i in range(len(probs[bii])):\n",
    "    print(index_to_char[i], probs[bii][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
