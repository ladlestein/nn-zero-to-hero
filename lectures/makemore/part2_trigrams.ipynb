{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the neural-network version of makemore for trigrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "with open(\"names.txt\") as file:\n",
    "    words = file.read().splitlines()\n",
    "words[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode all the words with integer values for the letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = \".abcdefghijklmnopqrstuvwxyz\"\n",
    "char_to_index = {}\n",
    "index = 0\n",
    "for char in list(chars):\n",
    "    char_to_index[char] = index\n",
    "    index += 1\n",
    "\n",
    "index_to_char = {index:char for char,index in char_to_index.items()}\n",
    "all_words_encoded = []\n",
    "for word in words:\n",
    "    letters = [\".\", \".\"] + list(word) + [\".\"]\n",
    "    all_words_encoded.append([char_to_index[letter] for letter in letters])\n",
    "\n",
    "def btoi(chars):\n",
    "    if len(chars) != 2:\n",
    "        raise Exception(\"bigram_to_index accepts only strings of two characters where both are either letters or \\\".\\\"\")\n",
    "    return char_to_index[chars[0]] * 27 + char_to_index[chars[1]]\n",
    "#\"\".join([index_to_char[char] for char in encoded_words[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network will have one layer, with one node for each possible two-character combination. Each will output the probability for each character that it will come next. We initialize it with random, normally distributed weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bigrams_possible = 27 * 27\n",
    "\n",
    "import torch\n",
    "torch.set_printoptions(linewidth=132)\n",
    "W = torch.randn(n_bigrams_possible, 27, requires_grad=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split all the words into trigrams. Represent the input (the first two characters) as the second + 27 * the first. Convert those all to one-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "\n",
    "for word_encoded in all_words_encoded[:]:\n",
    "    for index1, index2, index3 in zip(word_encoded, word_encoded[1:], word_encoded[2:]):\n",
    "        x = (index1 * 27) + index2\n",
    "        y = index3\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "inputs = F.one_hot(torch.tensor(xs), num_classes=n_bigrams_possible).float()\n",
    "\n",
    "n_observations = len(xs)\n",
    "n_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = inputs @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    logits = inputs @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(n_observations)].log().mean() + 0.01*(W**2).mean()\n",
    "    print(i, loss.item())\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -50 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "  print(i)\n",
    "  output = []\n",
    "  start_bigram = 0 # the bigram representing two word-starts characters.\n",
    "  start_bigram_encoded = F.one_hot(torch.tensor([start_bigram]), num_classes=n_bigrams_possible).float()\n",
    "  logits = start_bigram_encoded @ W\n",
    "  counts = logits.exp()\n",
    "  probs = counts / counts.sum(1, keepdims=True)\n",
    "  index = torch.multinomial(probs, 1, replacement=True).item()\n",
    "  \n",
    "  while True:\n",
    "    encoded = F.one_hot(torch.tensor([index]), num_classes=n_bigrams_possible).float()\n",
    "    logits = encoded @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    next_index = torch.multinomial(probs, 1, replacement=True).item()\n",
    "    next_char = index_to_char[next_index]\n",
    "    output.append(next_char)\n",
    "    if next_char == \".\":\n",
    "      break\n",
    "  print(\"\".join(output))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how many trigrams actually come up"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
