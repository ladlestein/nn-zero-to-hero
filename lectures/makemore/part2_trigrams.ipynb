{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the neural-network version of makemore for trigrams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "with open(\"names.txt\") as file:\n",
    "    words = file.read().splitlines()\n",
    "words[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From that we create the trigrams, which are the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'..'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words:\n\u001b[1;32m     11\u001b[0m     letters \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(word) \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m     encodings\u001b[39m.\u001b[39mappend([char_to_index[letter] \u001b[39mfor\u001b[39;49;00m letter \u001b[39min\u001b[39;49;00m letters])\n",
      "Cell \u001b[0;32mIn[138], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words:\n\u001b[1;32m     11\u001b[0m     letters \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(word) \u001b[39m+\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m     encodings\u001b[39m.\u001b[39mappend([char_to_index[letter] \u001b[39mfor\u001b[39;00m letter \u001b[39min\u001b[39;00m letters])\n",
      "\u001b[0;31mKeyError\u001b[0m: '..'"
     ]
    }
   ],
   "source": [
    "chars = \".abcdefghijklmnopqrstuvwxyz\"\n",
    "char_to_index = {}\n",
    "index = 0\n",
    "for char in list(chars):\n",
    "    char_to_index[char] = index\n",
    "    index += 1\n",
    "\n",
    "index_to_char = {index:char for char,index in char_to_index.items()}\n",
    "encodings = []\n",
    "for word in words:\n",
    "    letters = [\".\", \".\"] + list(word) + [\".\"]\n",
    "    encodings.append([char_to_index[letter] for letter in letters])\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network will have one layer, with one node for each possible two-character combination. Each will output the probability for each character that it will come next. We initialize it with random, normally distributed weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bigrams_possible = 27 * 27\n",
    "\n",
    "import torch\n",
    "W = torch.randn(n_bigrams_possible, 27, requires_grad=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split all the words into trigrams. Represent the input (the first two characters) as the second + 27 * the first. Convert those all to one-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = [], []\n",
    "for encoding in encodings:\n",
    "    for index1, index2, index3 in zip(encoding, encoding[1:], encoding[2:]):\n",
    "        x = (index1 * 27) + index2\n",
    "        y = index3\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "inputs = F.one_hot(torch.tensor(xs), num_classes=n_bigrams_possible).float()\n",
    "\n",
    "n_observations = len(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    logits = inputs @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(n_observations)].log().mean()\n",
    "    print(loss.item())\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -.5 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.one_hot(torch.arange(27), num_classes=n_bigrams_possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "  output = []\n",
    "  index = 0\n",
    "  first_possible_bigrams = F.one_hot(torch.arange(27), num_classes=n_bigrams_possible).float()\n",
    "  logits = first_possible_bigrams @ W\n",
    "  counts = logits.exp()\n",
    "  probs = counts / counts.sum(1, keepdims=True)\n",
    "  index = torch.multinomial(probs, 1, replacement=True).item()\n",
    "  \n",
    "  while True:\n",
    "    encoded = F.one_hot(torch.tensor(index), num_classes=n_bigrams_possible).float()\n",
    "    logits = encoded @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    next_index = torch.multinomial(probs, 1, replacement=True).item()\n",
    "    next_char = index_to_char[next_index]\n",
    "    output.append(next_char)\n",
    "    if next_char == \".\":\n",
    "      break\n",
    "  print(\"\".join(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
